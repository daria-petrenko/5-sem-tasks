{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47151,
     "status": "ok",
     "timestamp": 1543783910566,
     "user": {
      "displayName": "Дарья Петренко",
      "photoUrl": "",
      "userId": "16288013397647856242"
     },
     "user_tz": -180
    },
    "id": "kvo-mwxL7_fl",
    "outputId": "479cdd95-da9e-48b6-d554-4fb11de1034e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0y8QoU9Q8XX-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.special import logsumexp, expit\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UDRKtGV78afC"
   },
   "outputs": [],
   "source": [
    "class BaseSmoothOracle:\n",
    "\n",
    "    def func(self, w):\n",
    "        if(sparse.issparse(self.X)):\n",
    "            if(w.ndim == 1):\n",
    "                return np.sum(logsumexp(\n",
    "                    np.vstack((\n",
    "                            -1 * np.asarray(self.X.dot(w)) * self.y,\n",
    "                            np.zeros(np.size(self.y))\n",
    "                            )).T,\n",
    "                    axis=1\n",
    "                    )) / np.size(self.y) + self.l2_coef * np.dot(w, w) / 2\n",
    "            else:\n",
    "                max_val = np.amax(np.asarray(self.X.dot(w.T)), axis=1)\n",
    "                return -1 * np.sum(\n",
    "                    np.squeeze(np.asarray(\n",
    "                            self.X.multiply(w[self.y.astype(int)]).sum(axis=1)), axis=1) -\n",
    "                    max_val -\n",
    "                    logsumexp(\n",
    "                            np.asarray(self.X.dot(w.T)) -\n",
    "                            max_val[:, np.newaxis],\n",
    "                            axis=1)\n",
    "                    ) / np.size(self.y) + self.l2_coef * np.sum(w * w) / 2\n",
    "        else:\n",
    "            if(w.ndim == 1):\n",
    "                return np.sum(logsumexp(\n",
    "                        np.vstack((\n",
    "                                -1 * np.dot(self.X, w) * self.y,\n",
    "                                np.zeros(np.size(self.y))\n",
    "                                )).T,\n",
    "                        axis=1\n",
    "                        )) / np.size(self.y) + self.l2_coef * np.dot(w, w) / 2\n",
    "            else:\n",
    "                max_val = np.amax(np.dot(self.X, w.T), axis=1)\n",
    "                return -1 * np.sum(\n",
    "                        np.sum(self.X * w[self.y.astype(int)], axis=1) - max_val -\n",
    "                        logsumexp(\n",
    "                                np.dot(self.X, w.T) -\n",
    "                                max_val[:, np.newaxis],\n",
    "                                axis=1)\n",
    "                        ) / np.size(self.y) + self.l2_coef * np.sum(w * w) / 2\n",
    "\n",
    "    def grad(self, w):\n",
    "        if(self.X.dtype == int):\n",
    "            min_val = np.iinfo(self.X.dtype).min\n",
    "            max_val = np.iinfo(self.X.dtype).max\n",
    "        elif(self.X.dtype == float):\n",
    "            min_val = np.finfo(self.X.dtype).min\n",
    "            max_val = np.finfo(self.X.dtype).max\n",
    "        if(sparse.issparse(self.X)):\n",
    "            if(w.ndim == 1):\n",
    "                arg = np.asarray(self.X.dot(w)) * self.y\n",
    "                return np.squeeze(np.asarray(\n",
    "                    self.X.multiply(self.y[:, np.newaxis]).multiply(\n",
    "                        -1 * (np.clip(\n",
    "                                    np.exp(-1 * arg),\n",
    "                                    min_val, max_val\n",
    "                            ) * expit(arg))[:, np.newaxis]\n",
    "                    ).sum(axis=0)), axis=0) / \\\n",
    "                    np.size(self.y) + self.l2_coef * w\n",
    "            else:\n",
    "                mask = np.arange(np.size(w, 0))\n",
    "                mask = mask[:, np.newaxis] == self.y[np.newaxis, :]\n",
    "                max_arg = np.amax(np.asarray(self.X.dot(w.T)), axis=1)\n",
    "                arg = np.asarray(self.X.dot(w.T)) - max_arg[:, np.newaxis]\n",
    "                return self.X.transpose().dot( \n",
    "                    -1 * mask.T +\n",
    "                    np.clip(np.exp(arg), min_val, max_val) /\n",
    "                    np.clip(\n",
    "                        np.sum(np.exp(arg), axis=1),\n",
    "                        min_val, max_val)[:, np.newaxis]\n",
    "                ).T / np.size(self.y) + self.l2_coef * w\n",
    "        else:\n",
    "            if(w.ndim == 1):\n",
    "                arg = np.dot(self.X, w) * self.y\n",
    "                return np.sum(\n",
    "                        -1 * (np.clip(\n",
    "                                np.exp(-1 * arg),\n",
    "                                min_val, max_val\n",
    "                        ) * expit(arg))[:, np.newaxis] *\n",
    "                        self.X * self.y[:, np.newaxis],\n",
    "                        axis=0\n",
    "                        ) / np.size(self.y) + self.l2_coef * w\n",
    "            else: \n",
    "                mask = np.arange(np.size(w, 0))\n",
    "                mask = mask[:, np.newaxis] == self.y[np.newaxis, :]\n",
    "                max_arg = np.amax(np.dot(self.X, w.T), axis=1)\n",
    "                arg = np.dot(self.X, w.T) - max_arg[:, np.newaxis]\n",
    "                return np.dot(\n",
    "                    self.X.T, \n",
    "                    -1 * mask.T + \n",
    "                    np.clip(np.exp(arg), min_val, max_val) /\n",
    "                    np.clip(\n",
    "                        np.sum(np.exp(arg), axis=1),\n",
    "                        min_val, max_val\n",
    "                    )[:, np.newaxis]).T / np.size(self.y) + self.l2_coef * w\n",
    "                \n",
    "\n",
    "\n",
    "class BinaryLogistic(BaseSmoothOracle):\n",
    "\n",
    "    def __init__(self, l2_coef):\n",
    "        self.l2_coef = l2_coef\n",
    "\n",
    "    def func(self, X, y, w):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        return super().func(w)\n",
    "\n",
    "    def grad(self, X, y, w):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        return super().grad(w)\n",
    "\n",
    "\n",
    "class MulticlassLogistic(BaseSmoothOracle):\n",
    "\n",
    "    def __init__(self, l2_coef):\n",
    "        self.l2_coef = l2_coef\n",
    "\n",
    "    def func(self, X, y, w):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        return super().func(w)\n",
    "\n",
    "    def grad(self, X, y, w):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        return super().grad(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WLNS7bb6Ov6M"
   },
   "outputs": [],
   "source": [
    "class GDClassifier:\n",
    "\n",
    "    def __init__(self, loss_function, step_alpha=0.1, step_beta=1,\n",
    "                 tolerance=1e-5, max_iter=1000, **kwargs):\n",
    "        self.loss_function = loss_function\n",
    "        self.step_alpha = step_alpha\n",
    "        self.step_beta = step_beta\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def fit(self, X, y, X_test=np.zeros(1), y_test=np.zeros(1), w_0=None, trace=False):\n",
    "        if(self.loss_function == 'binary_logistic'):\n",
    "            if(w_0 is None):\n",
    "                w_0 = np.zeros(np.size(X, 1))\n",
    "            self.lr = BinaryLogistic(**self.kwargs)\n",
    "        elif(self.loss_function == 'multinomial_logistic'):\n",
    "            if(w_0 is None):\n",
    "                w_0 = np.zeros((np.size(np.unique(y)), np.size(X, 1)))\n",
    "            self.lr = MulticlassLogistic(**self.kwargs)\n",
    "        self.w = w_0.copy()\n",
    "        last_func = self.lr.func(X, y, self.w)\n",
    "        curr_func = last_func\n",
    "        if(trace):\n",
    "            self.history = dict()\n",
    "            self.history['time'] = [0.0]\n",
    "            self.history['func'] = [last_func]\n",
    "            self.history['acc'] = [np.sum(np.equal(y_test, self.predict(X_test))) / np.size(y_test)]\n",
    "            start = time.time()\n",
    "        num_iter = 0\n",
    "        while(num_iter == 0 or\n",
    "              (np.abs(curr_func - last_func) >= self.tolerance and\n",
    "               num_iter < self.max_iter)):\n",
    "            num_iter += 1\n",
    "            self.w -= self.lr.grad(X, y, self.w) * \\\n",
    "                self.step_alpha / num_iter ** self.step_beta\n",
    "            last_func = curr_func\n",
    "            curr_func = self.lr.func(X, y, self.w)\n",
    "            if(trace):\n",
    "                end = time.time()\n",
    "                self.history['time'].append(end - start)\n",
    "                self.history['func'].append(curr_func)\n",
    "                self.history['acc'].append(np.sum(np.equal(y_test, self.predict(X_test))) / np.size(y_test))\n",
    "        if(trace):\n",
    "            return self.history\n",
    "\n",
    "    def predict(self, X):\n",
    "        if(self.loss_function == 'binary_logistic'):\n",
    "            if(sparse.issparse(X)):\n",
    "                return np.sign(np.asarray(X.dot(self.w.T)))\n",
    "            else:\n",
    "                return np.sign(np.dot(X, self.w.T))\n",
    "        else:\n",
    "            if(sparse.issparse(X)):\n",
    "                return np.argmax(np.asarray(X.dot(self.w.T)), axis=1)\n",
    "            else:\n",
    "                return np.argmax(np.dot(X, self.w.T), axis=1)\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if(self.loss_function == 'binary_logistic'):\n",
    "            if(sparse.issparse(X)):\n",
    "                return expit(np.asarray(X.dot(self.w.T)))\n",
    "            else:\n",
    "                return expit(np.dot(X, self.w.T))\n",
    "        elif(self.loss_function == 'multinomial_logistic'):\n",
    "            if(sparse.issparse(X)):\n",
    "                softmax = np.exp(np.asarray(X.dot(self.w.T)) -\n",
    "                                 np.amax(\n",
    "                                         np.asarray(X.dot(self.w.T)),\n",
    "                                         axis=1\n",
    "                                         )[:, np.newaxis]\n",
    "                                 )\n",
    "            else:\n",
    "                softmax = np.exp(np.dot(X, self.w.T) -\n",
    "                                 np.amax(\n",
    "                                         np.dot(X, self.w.T),\n",
    "                                         axis=1\n",
    "                                         )[:, np.newaxis]\n",
    "                                 )\n",
    "            return softmax / np.sum(softmax, axis=1)[:, np.newaxis]\n",
    "\n",
    "    def get_objective(self, X, y):\n",
    "        return self.lr.func(X, y, self.w)\n",
    "\n",
    "    def get_gradient(self, X, y):\n",
    "        return self.lr.grad(X, y, self.w)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.w\n",
    "\n",
    "\n",
    "class SGDClassifier(GDClassifier):\n",
    "\n",
    "    def __init__(self, loss_function, batch_size, step_alpha=1, step_beta=0,\n",
    "                 tolerance=1e-5, max_iter=100000, random_seed=153, **kwargs):\n",
    "        GDClassifier.__init__(self, loss_function=loss_function, step_alpha=step_alpha, step_beta=step_beta,\n",
    "                             tolerance=tolerance, max_iter=max_iter, **kwargs)\n",
    "        self.loss_function = loss_function\n",
    "        self.batch_size = batch_size\n",
    "        self.step_alpha = step_alpha\n",
    "        self.step_beta = step_beta\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.random_seed = random_seed\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def fit(self, X, y, X_test=np.zeros(1), y_test=np.zeros(1), w_0=None, trace=False, log_freq=1):\n",
    "        np.random.seed(self.random_seed)\n",
    "        if(self.loss_function == 'binary_logistic'):\n",
    "            if(w_0 is None):\n",
    "                w_0 = np.zeros(np.size(X, 1))\n",
    "            self.lr = BinaryLogistic(**self.kwargs)\n",
    "        elif(self.loss_function == 'multinomial_logistic'):\n",
    "            if(w_0 is None):\n",
    "                w_0 = np.zeros((np.size(np.unique(y)), np.size(X, 1)))\n",
    "            self.lr = MulticlassLogistic(**self.kwargs)\n",
    "        self.w = w_0.copy()\n",
    "        last_func = self.lr.func(X, y, self.w)\n",
    "        curr_func = last_func\n",
    "        if(trace):\n",
    "            self.history = dict()\n",
    "            self.history['epoch_num'] = [0.0]\n",
    "            self.history['time'] = [0.0]\n",
    "            self.history['func'] = [last_func]\n",
    "            self.history['weights_diff'] = [0.0]\n",
    "            self.history['acc'] = [np.sum(np.equal(y_test, self.predict(X_test))) / np.size(y_test)]\n",
    "            start = time.time()\n",
    "            last_epoch_num = 0\n",
    "            curr_epoch_num = 0\n",
    "            last_w = self.w.copy()\n",
    "            curr_w = last_w.copy()\n",
    "        num_iter = 0\n",
    "        ind_list = np.arange(np.size(X, 0))\n",
    "        np.random.shuffle(ind_list)\n",
    "        curr_ind = 0\n",
    "        while(num_iter == 0 or\n",
    "              (np.abs(curr_func - last_func) >= self.tolerance and\n",
    "               num_iter < self.max_iter)):\n",
    "            if(curr_ind >= np.size(X, 0)):\n",
    "                np.random.shuffle(ind_list)\n",
    "                curr_ind = 0\n",
    "            num_iter += 1\n",
    "            self.w -= self.lr.grad(\n",
    "                    X[curr_ind:curr_ind + self.batch_size, :], \n",
    "                    y[curr_ind:curr_ind + self.batch_size], self.w\n",
    "                    ) * self.step_alpha / num_iter ** self.step_beta\n",
    "            last_func = curr_func.copy()\n",
    "            curr_func = self.lr.func(X, y, self.w)\n",
    "            if(trace):\n",
    "                if(curr_ind  + self.batch_size >= np.size(ind_list)):\n",
    "                    curr_epoch_num += (np.size(ind_list) - curr_ind) / \\\n",
    "                        np.size(ind_list)\n",
    "                else:\n",
    "                    curr_epoch_num += self.batch_size / np.size(ind_list)\n",
    "                if(curr_epoch_num - last_epoch_num >= log_freq):\n",
    "                    end = time.time()\n",
    "                    last_w = curr_w.copy()\n",
    "                    curr_w = self.w\n",
    "                    self.history['epoch_num'].append(curr_epoch_num)\n",
    "                    self.history['time'].append(end - start)\n",
    "                    self.history['func'] .append(curr_func)\n",
    "                    self.history['acc'].append(np.sum(np.equal(y_test, self.predict(X_test))) / np.size(y_test))\n",
    "                    self.history['weights_diff'].append(\n",
    "                            np.sum(\n",
    "                                    (last_w - curr_w) ** 2, \n",
    "                                    axis=-1))\n",
    "                    last_epoch_num = curr_epoch_num\n",
    "            curr_ind += self.batch_size\n",
    "        if(trace):\n",
    "            return self.history\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return super().predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1BGTgipyIIgS"
   },
   "outputs": [],
   "source": [
    "class MulticlassStrategy:   \n",
    "    def __init__(self, classifier, mode, **kwargs):\n",
    "        self.classifier = classifier\n",
    "        self.mode = mode\n",
    "        self.kwargs = kwargs\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.num_classes = np.size(np.unique(y))\n",
    "        if(self.mode == 'one_vs_all'):\n",
    "            self.w = np.zeros((self.num_classes, X.shape[1]))\n",
    "            for i in range(self.num_classes):\n",
    "                mask = y == i\n",
    "                curr_y = 2 * mask - 1 # 1 if y == i, -1 if not\n",
    "                curr_lr = self.classifier(**self.kwargs)\n",
    "                curr_lr.fit(X, curr_y)\n",
    "                self.w[i] = curr_lr.get_weights()\n",
    "        elif(self.mode == 'all_vs_all'):\n",
    "            self.w = np.zeros((self.num_classes * (self.num_classes - 1) // 2, \n",
    "                              X.shape[1]))\n",
    "            num = 0\n",
    "            i_list = []\n",
    "            j_list = []\n",
    "            for i in range(self.num_classes - 1):\n",
    "                for j in range(i + 1, self.num_classes):\n",
    "                    i_list.append(i)\n",
    "                    j_list.append(j)\n",
    "                    matr_i = y == i\n",
    "                    matr_j = y == j\n",
    "                    matr = np.logical_or(matr_i, matr_j)\n",
    "                    curr_x = X[matr]\n",
    "                    curr_y = y.copy()\n",
    "                    curr_y[matr_i] = 1\n",
    "                    curr_y[matr_j] = -1\n",
    "                    curr_y = curr_y[matr]\n",
    "                    curr_lr = self.classifier(**self.kwargs)\n",
    "                    curr_lr.fit(curr_x, curr_y)\n",
    "                    self.w[num] = curr_lr.get_weights() \n",
    "                    num += 1\n",
    "            self.i_arr = np.array(i_list)\n",
    "            self.j_arr = np.array(j_list)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        if(self.mode == 'one_vs_all'):\n",
    "            if(sparse.issparse(X)):\n",
    "                return np.argmax(np.asarray(X.dot(self.w.T)), axis=1)\n",
    "            else:\n",
    "                return np.argmax(np.dot(X, self.w.T), axis=1)\n",
    "        elif(self.mode == 'all_vs_all'):\n",
    "            if(sparse.issparse(X)):\n",
    "                pred = np.asarray(X.dot(self.w.T))\n",
    "            else:\n",
    "                pred = np.dot(X, self.w.T)\n",
    "            mask = pred > 0\n",
    "            self.i_arr = np.broadcast_to(self.i_arr, pred.shape)\n",
    "            self.j_arr = np.broadcast_to(self.j_arr, pred.shape)\n",
    "            pred[mask] = self.i_arr[mask]                            \n",
    "            mask = np.logical_not(mask)\n",
    "            pred[mask] = self.j_arr[mask]\n",
    "            return np.argmax(np.apply_along_axis(\n",
    "                lambda a:np.bincount(a, minlength=self.num_classes), \n",
    "                -1, pred.astype(int)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMvDPzPQ8ieJ"
   },
   "outputs": [],
   "source": [
    "with open('/content/gdrive/My Drive/task2/news_test.json') as data_file:    \n",
    "    test = pd.read_json(data_file)\n",
    "with open('/content/gdrive/My Drive/task2/news_train.json') as data_file:    \n",
    "    train = pd.read_json(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_bjILgPEXNa"
   },
   "outputs": [],
   "source": [
    "x_test = test['text'].values\n",
    "x_train = train['text'].values\n",
    "y_test = test['sentiment'].values\n",
    "y_train = train['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iVvz1Gx1FHnx"
   },
   "outputs": [],
   "source": [
    "for i in range(np.size(x_test)):\n",
    "    x_test[i] = x_test[i].lower()\n",
    "    x_test[i] = re.sub('[^0-9а-я]+', ' ', x_test[i])\n",
    "for i in range(np.size(x_train)):\n",
    "    x_train[i] = x_train[i].lower()\n",
    "    x_train[i] = re.sub('[^0-9а-я]+', ' ', x_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y0cVMVF7Ilq7"
   },
   "outputs": [],
   "source": [
    "y_train_mult = np.zeros(y_train.shape)\n",
    "neg_matr = y_train == 'negative'\n",
    "y_train_mult[neg_matr] = 0\n",
    "neutral_matr = y_train == 'neutral'\n",
    "y_train_mult[neutral_matr] = 1\n",
    "pos_matr = y_train == 'positive'\n",
    "y_train_mult[pos_matr] = 2\n",
    "\n",
    "y_test_mult = np.zeros(y_test.shape)\n",
    "neg_matr = y_test == 'negative'\n",
    "y_test_mult[neg_matr] = 0\n",
    "neutral_matr = y_test == 'neutral'\n",
    "y_test_mult[neutral_matr] = 1\n",
    "pos_matr = y_test == 'positive'\n",
    "y_test_mult[pos_matr] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3941,
     "status": "ok",
     "timestamp": 1543776267275,
     "user": {
      "displayName": "Дарья Петренко",
      "photoUrl": "",
      "userId": "16288013397647856242"
     },
     "user_tz": -180
    },
    "id": "C7_LCpKrIrE8",
    "outputId": "713fa33b-8af2-4a55-c73d-759b1c3086f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1240, 39335)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_df=5)\n",
    "x_train_mult = vectorizer.fit_transform(x_train_file)\n",
    "x_test_mult = vectorizer.transform(x_test_file)\n",
    "print(x_test_mult.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 531026,
     "status": "ok",
     "timestamp": 1543784584467,
     "user": {
      "displayName": "Дарья Петренко",
      "photoUrl": "",
      "userId": "16288013397647856242"
     },
     "user_tz": -180
    },
    "id": "LwPcQ3n9IwV8",
    "outputId": "1d4a644a-5571-461b-8cc8-7823aad3c65c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multinomial 0.6266129032258064\n",
      "wall time, sec: 530.5039427280426\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "lr = GDClassifier(loss_function='multinomial_logistic', step_alpha=1, step_beta=0.1, \n",
    "            tolerance=1e-4, l2_coef=0.001)\n",
    "lr.fit(x_train_mult, y_train_mult)\n",
    "y_pred = lr.predict(x_test_mult)\n",
    "print('multinomial', np.sum(np.equal(y_pred, y_test_mult)) / np.size(y_test_mult))\n",
    "end = time.time()\n",
    "print('wall time, sec:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 78646,
     "status": "ok",
     "timestamp": 1543766645549,
     "user": {
      "displayName": "Дарья Петренко",
      "photoUrl": "",
      "userId": "16288013397647856242"
     },
     "user_tz": -180
    },
    "id": "Gq3lwfUTIzqI",
    "outputId": "10bfae64-ba2c-4d4c-de5a-5271688ced87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all vs all 0.5701612903225807\n",
      "wall time, sec: 78.14098167419434\n"
     ]
    }
   ],
   "source": [
    "bin_lr = SGDClassifier\n",
    "args_dict = dict()\n",
    "args_dict['batch_size'] = 100\n",
    "args_dict['loss_function'] = 'binary_logistic'\n",
    "args_dict['max_iter'] = 1000\n",
    "args_dict['step_alpha'] = 0.1\n",
    "args_dict['step_beta'] = 0.1\n",
    "args_dict['l2_coef'] = 1\n",
    "start = time.time()\n",
    "lr = MulticlassStrategy(classifier=bin_lr, mode='all_vs_all', **args_dict)\n",
    "lr.fit(x_train_mult, y_train_mult)\n",
    "y_pred = lr.predict(x_test_mult)\n",
    "print('all vs all', np.sum(np.equal(y_pred, y_test_mult)) / np.size(y_test_mult))\n",
    "end = time.time()\n",
    "print('wall time, sec:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7HUfYFzBJbYz"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train_mult = vectorizer.fit_transform(x_train_file)\n",
    "x_test_mult = vectorizer.transform(x_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zX_3W4z7krXb"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lr = GDClassifier(loss_function='multinomial_logistic', step_alpha=10, step_beta=2, l2_coef=1)\n",
    "lr.fit(x_train_mult, y_train_mult, w_0=np.ones((3, np.size(x_train_mult, 1))))\n",
    "y_pred = lr.predict(x_test_mult)\n",
    "print('multinomial', np.sum(np.equal(y_pred, y_test_mult)) / np.size(y_test_mult))\n",
    "end = time.time()\n",
    "print('wall time, sec:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jtc_xCpnkuc5"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/content/gdrive/My Drive/task2/test.txt', 'rb') as data_file:    \n",
    "    x_test_file = pickle.load(data_file)\n",
    "    \n",
    "with open('/content/gdrive/My Drive/task2/train.txt', 'rb') as data_file:    \n",
    "    x_train_file = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 508,
     "status": "ok",
     "timestamp": 1543773904383,
     "user": {
      "displayName": "Дарья Петренко",
      "photoUrl": "",
      "userId": "16288013397647856242"
     },
     "user_tz": -180
    },
    "id": "oFwS3QjKTKJh",
    "outputId": "484b465c-5ab3-403b-d2fb-63b391eadc01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1240, 56532)"
      ]
     },
     "execution_count": 192,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_mult.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 602,
     "status": "ok",
     "timestamp": 1543781161818,
     "user": {
      "displayName": "Дарья Петренко",
      "photoUrl": "",
      "userId": "16288013397647856242"
     },
     "user_tz": -180
    },
    "id": "p5IBFVtKTPop",
    "outputId": "3a1a663a-0bc6-462a-e4bb-e88be9312ce9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 85, 106,  24],\n",
       "       [ 22, 475, 108],\n",
       "       [  1, 143, 276]])"
      ]
     },
     "execution_count": 248,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test_mult, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 506,
     "status": "ok",
     "timestamp": 1543782231037,
     "user": {
      "displayName": "Дарья Петренко",
      "photoUrl": "",
      "userId": "16288013397647856242"
     },
     "user_tz": -180
    },
    "id": "wauUPe_LgMXR",
    "outputId": "3a750cef-159e-4373-d082-cc5fe3692215"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "\n",
      "\n",
      "\n",
      "По данным союза предприятий автомобильной отрасли Казахстана «КазАвтоПром» за  9 месяцев 2016 года  казахстанцы приобрели 30,8 тыс. новых  легковых автомобилей на сумму $569 млн, что на 59% ниже аналогичного показателя минувшего года.\n",
      "По результатам января-сентября официальными дилерами было реализовано 30 759  легковых и легких коммерческих автомобилей, что на 59% ниже аналогичного показателя минувшего года (74 911 ед.). Продажи в стоимостном выражении в отчетный период составили $569 млн., уступив 58,7% значению соответствующего периода 2015 года.\n",
      "В сентябре объем продаж сократился до отметки 3 207 автомобилей против 8129 ед. в том же месяце 2015 г. (-60%).\n",
      "Отрицательная динамика сбыта на автомобильном рынке республики сохраняется начиная со II квартала 2014 года. Наряду с другими сегментами казахстанского ретейла авторынок продолжает испытывать негативные последствия девальвации национальной валюты и снижения доступности кредитных ресурсов.\n",
      "Казахстанский автопарк «помолодел»… По информации аналитической службы Energyprom.kz на 1 сентября доля машин возрастом менее 3 лет составляла 16,2%. Для сравнения: к осени 2015 нов…\n",
      "23 568 автомобилей (76%), реализованных по итогам 9 месяцев, пришлось на импорт, 7154 покупателя (24%) остановили выбор на продукции казахстанских автозаводов.\n",
      "Рейтинг наиболее востребованных казахстанцами брендов по итогам отчетного периода возглавляет Lada: за 9 месяцев 2016 г. на территории республики продано 6 086 автомобилей российской марки. Сбыт дилеров Toyota составил 5 728 единиц. Тройку лидеров замыкает Renault с показателем 2 925 реализованных автомобилей. В десятку наиболее успешных брендов вошли также Hyundai (2 659 ед.), KIA (2 325 ед.), Volkswagen (1583 ед.), GAZ (1 467 ед.), UAZ (1 196 ед.), Nissan (1 103 ед.) и Chevrolet (644 ед,).\n",
      "Лидерство в рейтинге моделей сохраняет Lada Priora. Российский седан предпочли 2 237 покупателей. Пятерку бестселлеров составили также Toyota Camry (2 232 ед.), Lada 4×4 (1 534 ед.), Renault Duster (1394 ед.) и Hyundai Accent (1261 ед.). Среди моделей, вошедших в TOP-10 потребительских предпочтений, также Volkswagen Polo (1236 ед.), Lada Granta (1164 ед.), KIA Rio(1015 ед.), Renault Sandero (1012 ед.) и Toyota Land Cruiser Prado (916 ед.).\n",
      "Статус крупнейшей дилерской группы на рынке Казахстана удерживает холдинг «БИПЭК АВТО – АЗИЯ АВТО». По итогам отчетного период 33,3% всех покупателей новых легковых автомобилей (включая LCV) остановили выбор на предложениях этой компании. 20,6% рынка пришлось на сбыт «Toyota Motor Kazakhstan». Доля КМК «Astana Motors» составила 9,7%. В десятку наиболее заметных участников казахстанского авторетейла вошли также компании «Вираж» (доля – 7,4%), «Mercur Auto» (5,3%), «Aster Auto» (3,8%), «Урал-Кров Авто» (2,7%), «Allur Auto» (2,6%), «Автомир» (2%), «Astek-Auto» (1,5%).\n",
      "Наибольшую активность в приобретении легковых автомобилей за этот период  проявили жители Алматы (9281 реализованных авто), Астаны(5333 ед.), Атырау (2234 ед.), Усть-Каменогорска (1656 ед.), Караганды (1623 ед.), Шымкента  (1573 ед.), Актау (1370 ед.), Костаная (1368 ед.), Уральска (1320 ед.) и Актобе (1196 ед.).\n",
      " Источник: Курсивъ\n",
      "Поделится в WhatsApp\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mask_2 = y_pred == 2\n",
    "mask_0 = y_test_mult == 0\n",
    "mask = np.logical_and(mask_2, mask_0)\n",
    "print(np.sum(mask))\n",
    "print(x_test[mask][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jyFO4cEkgqfS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "task_2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
